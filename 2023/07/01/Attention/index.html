<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet"><meta charset="utf-8"><title>自注意力机制做了什么？ | Have a Nice Journey</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="LLM"><meta name="description" content="在听了很多关于 自注意力机制 的描述后，不自觉的会想到它用什么数学方式，模仿人类思考、关注重点词汇、短语呢？这里我打算记录一下个人理解。当遇到一句话时：”The animal didn’t cross the street because it was too tired”it 代表什么呢？人类一下子就能理解，但是计算机需要经过一系列计算，识别出it在这句话中的意义、重要程度。当模型处理每个词语（"><meta name="keywords" content="LLM"><meta property="og:type" content="article"><meta property="og:title" content="自注意力机制做了什么？"><meta property="og:url" content="http://yoursite.com/2023/07/01/Attention/index.html"><meta property="og:site_name" content="Have a Nice Journey"><meta property="og:description" content="在听了很多关于 自注意力机制 的描述后，不自觉的会想到它用什么数学方式，模仿人类思考、关注重点词汇、短语呢？这里我打算记录一下个人理解。当遇到一句话时：”The animal didn’t cross the street because it was too tired”it 代表什么呢？人类一下子就能理解，但是计算机需要经过一系列计算，识别出it在这句话中的意义、重要程度。当模型处理每个词语（"><meta property="og:locale" content="en"><meta property="og:image" content="https://bjdzliu.oss-cn-beijing.aliyuncs.com/hexo_images/ailab/attention.png"><meta property="og:updated_time" content="2025-01-22T15:30:10.356Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="自注意力机制做了什么？"><meta name="twitter:description" content="在听了很多关于 自注意力机制 的描述后，不自觉的会想到它用什么数学方式，模仿人类思考、关注重点词汇、短语呢？这里我打算记录一下个人理解。当遇到一句话时：”The animal didn’t cross the street because it was too tired”it 代表什么呢？人类一下子就能理解，但是计算机需要经过一系列计算，识别出it在这句话中的意义、重要程度。当模型处理每个词语（"><meta name="twitter:image" content="https://bjdzliu.oss-cn-beijing.aliyuncs.com/hexo_images/ailab/attention.png"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/titillium-web/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.0.3/jquery.min.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"></head></html><body><div id="wrap"><header id="header"><div id="header-outer" class="outer"><div class="container"><div class="container-inner"><div id="header-title"><h1 class="logo-wrap"><a href="/" class="logo"></a></h1></div><div id="header-inner" class="nav-container"><a id="main-nav-toggle" class="nav-icon fa fa-bars"></a><div class="nav-container-inner"><ul id="main-nav"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/">Home</a></li><ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Technical-Notes/">Technical Notes</a></li></ul><li class="main-nav-list-item"><a class="main-nav-list-link" href="/about/index.html">About</a></li></ul><nav id="sub-nav"><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></nav></div></div></div></div></div></header><div class="container"><div class="main-body container-inner"><div class="main-body-inner"><section id="main"><div class="main-body-header"><h1 class="header"><a class="page-title-link" href="/categories/Technical-Notes/">Technical Notes</a></h1></div><div class="main-body-content"><article id="post-Attention" class="article article-single article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">自注意力机制做了什么？</h1></header><div class="article-meta"><div class="article-date"><a href="/2023/07/01/Attention/" class="article-date"><time datetime="2023-07-01T16:22:00.000Z" itemprop="datePublished">2023-07-01</time></a></div><div class="article-tag"><i class="fa fa-tag"></i> <a class="tag-link" href="/tags/LLM/">LLM</a></div></div><div class="article-entry" itemprop="articleBody"><p>在听了很多关于 自注意力机制 的描述后，不自觉的会想到它用什么数学方式，模仿人类思考、关注重点词汇、短语呢？<br>这里我打算记录一下个人理解。</p><p>当遇到一句话时：<br>”The animal didn’t cross the street because it was too tired”<br>it 代表什么呢？人类一下子就能理解，但是计算机需要经过一系列计算，识别出it在这句话中的意义、重要程度。</p><p>当模型处理每个词语（在输入序列中的每个位置）的时候，自我注意力机制允许它检查输入序列中的其他位置，寻找可能有助于更好理解这个词语的线索。</p><p>在 Transformer 架构中，Encoder 和 Decoder 中的每一个小模块，都代表着一堆数学公式，Multi-Head Attention 也不例外<br><img src="https://bjdzliu.oss-cn-beijing.aliyuncs.com/hexo_images/ailab/attention.png" alt="Transformer "></p><p>既然是数学公式，就有输入，输出，结合注意力机制的描述，我们想测算“输入” 的内容是否重要，是否值得关注，那么就用 “输出” 进行打分，分数高的，权重就大。</p><p>注意力机制在给予足够的计算资源的情况下，有无限的参考窗口，因此能够在生成文本时使用故事的整个上下文。门控循环单元 (GRU) 和长短期记忆 (LSTM) 也有窗口，但是是有限的。<br>从更长的窗口中获取单词后，就开始计算相关性。</p><p>在注意力机制中，通常存在三个关键元素：查询（query）、键（key）、和值（value）。它们各自的角色和作用如下：<br>查询（query）：输入的内容，比如你在google中输入的查询问题<br>键（key）：输入序列中其他词语信息向量信息。理解成google搜索结果中的标题，标题有的和查询有关，有的不太相关。<br>值（value）： 输入序列中词语的内容。理解成google搜索结果中的content</p><p>Query vector, a Key vector, and a Value vector 如何得到？<br>Q,K,V分别由 “词向量” 乘以WQ, WK, WV得到。</p><p>WQ, WK, WV 如何得到？<br>三个矩阵是训练出来的</p><p>补充背景知识：<br>两个向量的相似度可以用点积计算<br><img src="https://bjdzliu.oss-cn-beijing.aliyuncs.com/hexo_images/ailab/attention_simi.jpg" alt="Func "><br>完全不相关=垂直=0</p><p>计算公式：<br><img src="https://bjdzliu.oss-cn-beijing.aliyuncs.com/hexo_images/ailab/func_attention.jpg" alt="Func "></p><p>根据输入数据的不同，查询（queries）、键（keys）和值（values）可以以各种方式表示，例如向量、矩阵或张量。在自然语言处理（NLP）的背景下，query 和 key 通常被表示为词嵌入（word embeddings），而 value 则表示为上下文嵌入（contextualized embeddings）。</p><p>计算过程基本如下（了解即可）：</p><ul><li>三个权重矩阵（W(Q)、W(K)、W(V)），初始化是随机的</li><li>第一步是将每个编码器输入向量与我们在训练过程中训练的三个权重矩阵（W(Q)、W(K)、W(V)）相乘。该矩阵乘法将为每个输入向量提供三个向量：键向量、查询向量和值向量。</li><li>K向量和Q向量点积</li><li>在第三步中，我们将得分除以关键向量 (d k )维度的平方根。论文中关键向量的维数是64，所以是8。背后的原因是如果点积变大，这会使得 Softmax 的输出接近于 0 或 1，而梯度较小，从而可能导致梯度消失的问题。除以(d k ),缩小点积。</li><li>在第四步中，我们将对查询词（这里是第一个词）计算的所有自注意力分数应用 softmax 函数。</li><li>在第五步中，我们将值向量乘以我们在上一步中计算的向量。</li><li>在最后一步中，我们对上一步中获得的加权值向量求和，这将为我们提供给定单词的自注意力输出。</li></ul><p>以上计算，让模型考虑了所有序列的位置。</p><p>在最后一步中，一旦softmax和v相乘，将过滤出高注意力部分(更值得被关注)，也就是说，得到了类似人脑看到的最重要的图像，文字。</p><p>参考：<br><a href="https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d" target="_blank" rel="noopener">Attention Networks</a><br><a href="https://www.geeksforgeeks.org/self-attention-in-nlp/" target="_blank" rel="noopener">Attention in NLP</a></p></div><footer class="article-footer"><a data-url="http://yoursite.com/2023/07/01/Attention/" data-id="cm68296xx0006hrqczg6xs7er" class="article-share-link"><i class="fa fa-share"></i>Share</a><script>(i=>{i("body").on("click",function(){i(".article-share-box.on").removeClass("on")}).on("click",".article-share-link",function(t){t.stopPropagation();var e=(t=i(this)).attr("data-url"),a=encodeURIComponent(e),o="article-share-box-"+t.attr("data-id"),t=t.offset();if(i("#"+o).length){if((r=i("#"+o)).hasClass("on"))return void r.removeClass("on")}else{var o=['<div id="'+o+'" class="article-share-box">','<input class="article-share-input" value="'+e+'">','<div class="article-share-links">','<a href="https://twitter.com/intent/tweet?url='+a+'" class="article-share-twitter" target="_blank" title="Twitter"></a>','<a href="https://www.facebook.com/sharer.php?u='+a+'" class="article-share-facebook" target="_blank" title="Facebook"></a>','<a href="http://pinterest.com/pin/create/button/?url='+a+'" class="article-share-pinterest" target="_blank" title="Pinterest"></a>','<a href="https://plus.google.com/share?url='+a+'" class="article-share-google" target="_blank" title="Google+"></a>',"</div>","</div>"].join(""),r=i(o);i("body").append(r)}i(".article-share-box.on").hide(),r.css({top:t.top+25,left:t.left}).addClass("on")}).on("click",".article-share-box",function(t){t.stopPropagation()}).on("click",".article-share-box-input",function(){i(this).select()}).on("click",".article-share-box-link",function(t){t.preventDefault(),t.stopPropagation(),window.open(this.href,"article-share-box-window-"+Date.now(),"width=500,height=450")})})(jQuery)</script></footer></div></article><section id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></div></section><aside id="sidebar"><a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a><nav id="article-nav"><a href="/2023/12/25/fn_rag/" id="article-nav-newer" class="article-nav-link-wrap"><strong class="article-nav-caption">newer</strong><p class="article-nav-title">某销售场景下如何利用LLM</p><i class="icon fa fa-chevron-right" id="icon-chevron-right"></i> </a><a href="/2023/06/30/AIGC_术语_A_Z/" id="article-nav-older" class="article-nav-link-wrap"><strong class="article-nav-caption">older</strong><p class="article-nav-title">AGI_Glossary术语</p><i class="icon fa fa-chevron-left" id="icon-chevron-left"></i></a></nav><div class="widgets-container"><div class="widget-wrap"><h3 class="widget-title">recents</h3><div class="widget"><ul id="recent-post" class="no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/2024/07/15/Agent-Tuning/" class="title">Agent-Tuning</a></p><p class="item-date"><time datetime="2024-07-15T20:33:00.000Z" itemprop="datePublished">2024-07-15</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/2024/07/14/Estimating-GPU-Memory/" class="title">估算GPU内存</a></p><p class="item-date"><time datetime="2024-07-14T19:00:00.000Z" itemprop="datePublished">2024-07-14</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/2024/02/20/fine_tune_part1/" class="title">总结Fine-Tune ChatGLM3的过程part-1</a></p><p class="item-date"><time datetime="2024-02-20T14:11:00.000Z" itemprop="datePublished">2024-02-20</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/2023/12/25/fn_rag/" class="title">某销售场景下如何利用LLM</a></p><p class="item-date"><time datetime="2023-12-25T12:11:00.000Z" itemprop="datePublished">2023-12-25</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/2023/07/01/Attention/" class="title">自注意力机制做了什么？</a></p><p class="item-date"><time datetime="2023-07-01T16:22:00.000Z" itemprop="datePublished">2023-07-01</time></p></div></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">categories</h3><div class="widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Technical-Notes/">Technical Notes</a><span class="category-list-count">23</span></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">archives</h3><div class="widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">tags</h3><div class="widget"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/">LLM</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance/">Performance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/aliyun/">aliyun</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azure/">azure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ceph/">ceph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/">k8s</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/network/">network</a><span class="tag-list-count">2</span></li></ul></div></div></div></aside></div></div></div><footer id="footer"><div class="container"><div class="container-inner"><a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a><div class="credit"><h1 class="logo-wrap"><a href="/" class="logo"></a></h1><p>&copy; 2025 bjdzliu</p></div></div></div></footer><script>var disqus_shortname="hexo-theme-hueman",disqus_url="http://yoursite.com/2023/07/01/Attention/";(()=>{var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script src="/js/main.js"></script></div></body>