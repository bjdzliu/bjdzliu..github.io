<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>总结Fine-Tune ChatGLM3的过程part-1 | Have a Nice Journey</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="LLM"><meta name="description" content="了解一些训练过程中必备的知识点买一个公有云GPU服务器，了解训练过程回顾基础知识模型量化、计算机计算精度：FP32 使用32位存储一个数字FP16 16bit存一个数字。其中1位为符号，5位为指数，10位为尾数。INT8是 FP32的量化版本，其中浮点数用8位整数近似。（还是有计算过程的，不是随便舍弃，INT8可以反向推导出FP32）n-gram:统计语言模型，用于预测下一个词的概率。利用机率大小"><meta name="keywords" content="LLM"><meta property="og:type" content="article"><meta property="og:title" content="总结Fine-Tune ChatGLM3的过程part-1"><meta property="og:url" content="http://yoursite.com/fine_tune_part1/index.html"><meta property="og:site_name" content="Have a Nice Journey"><meta property="og:description" content="了解一些训练过程中必备的知识点买一个公有云GPU服务器，了解训练过程回顾基础知识模型量化、计算机计算精度：FP32 使用32位存储一个数字FP16 16bit存一个数字。其中1位为符号，5位为指数，10位为尾数。INT8是 FP32的量化版本，其中浮点数用8位整数近似。（还是有计算过程的，不是随便舍弃，INT8可以反向推导出FP32）n-gram:统计语言模型，用于预测下一个词的概率。利用机率大小"><meta property="og:locale" content="en"><meta property="og:updated_time" content="2024-03-05T06:57:39.626Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="总结Fine-Tune ChatGLM3的过程part-1"><meta name="twitter:description" content="了解一些训练过程中必备的知识点买一个公有云GPU服务器，了解训练过程回顾基础知识模型量化、计算机计算精度：FP32 使用32位存储一个数字FP16 16bit存一个数字。其中1位为符号，5位为指数，10位为尾数。INT8是 FP32的量化版本，其中浮点数用8位整数近似。（还是有计算过程的，不是随便舍弃，INT8可以反向推导出FP32）n-gram:统计语言模型，用于预测下一个词的概率。利用机率大小"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/titillium-web/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.0.3/jquery.min.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"></head></html><body><div id="wrap"><header id="header"><div id="header-outer" class="outer"><div class="container"><div class="container-inner"><div id="header-title"><h1 class="logo-wrap"><a href="/" class="logo"></a></h1></div><div id="header-inner" class="nav-container"><a id="main-nav-toggle" class="nav-icon fa fa-bars"></a><div class="nav-container-inner"><ul id="main-nav"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/">Home</a></li><ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Technical-Notes/">Technical Notes</a></li></ul><li class="main-nav-list-item"><a class="main-nav-list-link" href="/about/index.html">About</a></li></ul><nav id="sub-nav"><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></nav></div></div></div></div></div></header><div class="container"><div class="main-body container-inner"><div class="main-body-inner"><section id="main"><div class="main-body-header"><h1 class="header"><a class="page-title-link" href="/categories/Technical-Notes/">Technical Notes</a></h1></div><div class="main-body-content"><article id="post-fine_tune_part1" class="article article-single article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">总结Fine-Tune ChatGLM3的过程part-1</h1></header><div class="article-meta"><div class="article-date"><a href="/fine_tune_part1/" class="article-date"><time datetime="2024-02-20T14:11:00.000Z" itemprop="datePublished">2024-02-20</time></a></div><div class="article-tag"><i class="fa fa-tag"></i> <a class="tag-link" href="/tags/LLM/">LLM</a></div></div><div class="article-entry" itemprop="articleBody"><p>了解一些训练过程中必备的知识点<br>买一个公有云GPU服务器，了解训练过程</p><h3 id="回顾基础知识"><a href="#回顾基础知识" class="headerlink" title="回顾基础知识"></a>回顾基础知识</h3><p>模型量化、计算机计算精度：<br>FP32 使用32位存储一个数字<br>FP16 16bit存一个数字。其中1位为符号，5位为指数，10位为尾数。<br>INT8是 FP32的量化版本，其中浮点数用8位整数近似。（还是有计算过程的，不是随便舍弃，INT8可以反向推导出FP32）</p><p>n-gram:<br>统计语言模型，用于预测下一个词的概率。<br>利用机率大小來判断句子合不合理</p><p>n是切词的个数。比如<br>n=2, 叫 bigram。”I love learning about deep learning.”。这句话的bigrams将包括：”I love”，”love learning”<br>n=3，叫trigram</p><p>SLOT和BLEU的指标:<br>槽填充（Slot Filling）: 我从上海飞往北京。上海和北京是slot value。slot是离开城市和抵达城市<br>BLEU：精确匹配n-grams来测量句子相似度。机器翻译的结果与一个或多个参考翻译。</p><p>Transform模型中：<br>Token Embedding+Position Embedding<br>每次将 input 序列都将经过这两层Embedding<br>Token Embedding：每一行都是一个word embedding，a list of number<br>Position Embedding： 当前位置和其他位置 ，进行sin or cos 的结果，整个序列形成一个矩阵。</p><p>Pre-training：通用数据训练出来的模型</p><p>Instruction tuning: 明确的问题及明确的答案。特指利用 pairs of input-output instructions 调优模型。目的：遵循自然语言指令，显著提高其在一系列任务中的有用性和适用性。也是一种Fine-Tune。</p><p>PEFT：仅微调少量 (注入额外的) 模型参数，同时冻结预训练 LLM 的大部分参数。目的：减少全参优化造成的资源消耗。本次实操过程属于PEFT。简要说一下在哪里注入参数： 在各个神经网络层，GPT3.5有96层，那么就在每一层，添加参数。</p><p>PEFT：</p><ul><li>prompt-tuning 用新初始化的embedding，加入到数据集中，修改输入序列。在输入层修改。</li><li>prefix-tuning 在神经单元网络中，每个隐层添加参数。</li><li>LoRA 训练低秩矩阵(AXB),添加到W(Q)、 W(V)上。</li><li>QLoRA 用模型量化，减少了参数的精度。</li></ul><p>GPT-3 模型由两个组件组成： The Model Body and the Head.<br>LLM Head ：可能是一个线性层，用 softmax 函数激活，为词汇表中的每个单词生成概率。</p><h3 id="云GPU服务器资源"><a href="#云GPU服务器资源" class="headerlink" title="云GPU服务器资源"></a>云GPU服务器资源</h3><p>NVIDIA 显卡上的几种类型的核心：</p><ul><li>CUDA 核心：最通用的核心，适用于各种计算任务。</li><li>张量核心：针对某些机器学习计算进行了优化</li><li>光线追踪 (RT) 核心：对于游戏来说比大多数机器学习更重要，这些核心专门用于模拟光的行为。</li></ul><p>VRAM:是显卡上的内存量</p><p>125 teraflops = 125万亿次浮点运算<br>万亿次浮点运算是指处理器每秒计算一兆个浮点运算的能力。例如，说某个东西有“6 TFLOPS”，意味着它的处理器设置平均每秒可以处理6万亿个浮点计算。</p><h4 id="显卡介绍："><a href="#显卡介绍：" class="headerlink" title="显卡介绍："></a>显卡介绍：</h4><p>NVIDIA Tesla T4 是一款中端数据中心 GPU。它于2019年发布。</p><ul><li>CUDA 核心：2560</li><li>张量核心：320</li><li>显存：16GiB<br>规格页面；<a href="https://www.nvidia.com/en-us/data-center/tesla-t4/" target="_blank" rel="noopener">https://www.nvidia.com/en-us/data-center/tesla-t4/</a></li></ul><p>A10 是比 T4 更大、更强大的 GPU，于 2021 年发布，采用 NVIDIA 的 Ampere 架构。</p><ul><li>CUDA 核心：9216</li><li>张量核心：288</li><li>显存：24 GiB</li></ul><h4 id="腾讯云"><a href="#腾讯云" class="headerlink" title="腾讯云"></a>腾讯云</h4><p>选择实例： PNV4.7XLARGE116</p><ul><li>自动安装或者手动安装驱动</li><li>手动安装 Tesla Driver + CUDA<br>下载 NVIDIA Tesla驱动<br><a href="https://cloud.tencent.com/document/product/560/8048" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/560/8048</a></li></ul><ul><li><p>安装python3.9</p></li><li><p>安装pip3.9</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</span><br><span class="line">python3.9 get-pip.py</span><br></pre></td></tr></table></figure></li><li><p>安装git-lfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git-lfs !git lfs install</span><br></pre></td></tr></table></figure></li></ul><h3 id="下载Model"><a href="#下载Model" class="headerlink" title="下载Model"></a>下载Model</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">GIT_LFS_SKIP_SMUDGE=1 git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git</span><br><span class="line">cd chatglm3-6b</span><br><span class="line">git lfs fetch --include="tokenizer.model"</span><br><span class="line">git lfs checkout tokenizer.model</span><br><span class="line">git lfs fetch --include="*.safetensors"</span><br><span class="line">git lfs checkout *.safetensors</span><br></pre></td></tr></table></figure><h3 id="了解数据集"><a href="#了解数据集" class="headerlink" title="了解数据集"></a>了解数据集</h3><h4 id="chatglm3-6b的对话格式"><a href="#chatglm3-6b的对话格式" class="headerlink" title="chatglm3-6b的对话格式"></a>chatglm3-6b的对话格式</h4><p>-&lt;|system|&gt; 提示器<br>-&lt;|user|&gt; 输入<br>-&lt;|assistant|&gt; 模型回复<br>-&lt;|observation|&gt; 工具调用、代码执行的结果</p><p>多轮对话的json格式表示,按照以下格式准备：训练数据集和验证数据集 train.json 和 dev.json文件</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"tools"</span>: [</span><br><span class="line">      <span class="string">"webTool:根据衣服查找哪个网站便宜\nParameters: &#123;\"name"</span>:\<span class="string">"衣服名称\",\"price\":\"衣服售价"</span>&#125;\nOutput:A url string that can tell user where is the cloth is on-sale<span class="string">"</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    "</span>conversations<span class="string">": [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>role<span class="string">": "</span>user<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>content<span class="string">": "</span>类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞<span class="string">"</span></span><br><span class="line"><span class="string">      &#125;,</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>role<span class="string">": "</span>assistant<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>content<span class="string">": "</span>简约而不简单的牛仔外套，白色的衣身十分百搭。<span class="string">"</span></span><br><span class="line"><span class="string">      &#125;,</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>role<span class="string">": "</span>user<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>content<span class="string">": "</span>在哪买<span class="string">"</span></span><br><span class="line"><span class="string">      &#125;,</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>role<span class="string">": "</span>tool<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>name<span class="string">": "</span>search_hotels<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>parameters<span class="string">": &#123;</span></span><br><span class="line"><span class="string">          "</span>name<span class="string">": "</span>牛仔外套<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>price<span class="string">": 300</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        "</span>observation<span class="string">": [</span></span><br><span class="line"><span class="string">          &#123;</span></span><br><span class="line"><span class="string">            "</span>website<span class="string">": "</span>https://example.com<span class="string">"</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">]</span></span><br></pre></td></tr></table></figure><p>ChatGLM3-6b原生支持function call<br>tools :表示function和parameters的定义<br>conversations： 表示多轮对话<br>user 和 assistant ： 分别是用户和AI的问答。<br>角色带loss字段。没写默认对 system, user 不计算 loss，其余角色则计算 loss。</p><h3 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h3><p>这里参照 <a href="https://github.com/THUDM/ChatGLM3/blob/main/finetune_demo/lora_finetune.ipynb" target="_blank" rel="noopener">github</a> 操作即可</p><h3 id="在单卡上运行"><a href="#在单卡上运行" class="headerlink" title="在单卡上运行"></a>在单卡上运行</h3><p>用官方的代码,花了5个小时</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd finetune_demo</span><br><span class="line">python finetune_hf.py  data/AdvertiseGen/  THUDM/chatglm3-6b  configs/lora.yaml</span><br></pre></td></tr></table></figure><p>生成如下内容,使用新模型，加载最新的目录即可。大小在几百兆。我的理解是lora训练的新参数，依赖于原始的Model（12G）文件（别删了）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x  3 root root 4096 Mar  1 15:33 test_lora-20240301-153316</span><br><span class="line">drwxr-xr-x  3 root root 4096 Mar  1 15:37 test_lora-20240301-153719</span><br><span class="line">drwxr-xr-x  3 root root 4096 Mar  1 15:49 test_lora-20240301-154939</span><br><span class="line">drwxr-xr-x  3 root root 4096 Mar  1 16:09 test_lora-20240301-160948</span><br><span class="line">drwxr-xr-x 13 root root 4096 Mar  1 21:47 test_lora-20240301-161128</span><br></pre></td></tr></table></figure></div><footer class="article-footer"><a data-url="http://yoursite.com/fine_tune_part1/" data-id="clte0pxbp000rdzq6bw9i9eoa" class="article-share-link"><i class="fa fa-share"></i>Share</a><script>!function(i){i("body").on("click",function(){i(".article-share-box.on").removeClass("on")}).on("click",".article-share-link",function(t){t.stopPropagation();var e=(t=i(this)).attr("data-url"),a=encodeURIComponent(e),o="article-share-box-"+t.attr("data-id"),t=t.offset();if(i("#"+o).length){if((r=i("#"+o)).hasClass("on"))return void r.removeClass("on")}else{var o=['<div id="'+o+'" class="article-share-box">','<input class="article-share-input" value="'+e+'">','<div class="article-share-links">','<a href="https://twitter.com/intent/tweet?url='+a+'" class="article-share-twitter" target="_blank" title="Twitter"></a>','<a href="https://www.facebook.com/sharer.php?u='+a+'" class="article-share-facebook" target="_blank" title="Facebook"></a>','<a href="http://pinterest.com/pin/create/button/?url='+a+'" class="article-share-pinterest" target="_blank" title="Pinterest"></a>','<a href="https://plus.google.com/share?url='+a+'" class="article-share-google" target="_blank" title="Google+"></a>',"</div>","</div>"].join(""),r=i(o);i("body").append(r)}i(".article-share-box.on").hide(),r.css({top:t.top+25,left:t.left}).addClass("on")}).on("click",".article-share-box",function(t){t.stopPropagation()}).on("click",".article-share-box-input",function(){i(this).select()}).on("click",".article-share-box-link",function(t){t.preventDefault(),t.stopPropagation(),window.open(this.href,"article-share-box-window-"+Date.now(),"width=500,height=450")})}(jQuery)</script></footer></div></article><section id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></div></section><aside id="sidebar"><a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a><nav id="article-nav"><a href="/fn_rag/" id="article-nav-older" class="article-nav-link-wrap"><strong class="article-nav-caption">older</strong><p class="article-nav-title">某销售场景下如何利用LLM</p><i class="icon fa fa-chevron-left" id="icon-chevron-left"></i></a></nav><div class="widgets-container"><div class="widget-wrap"><h3 class="widget-title">recents</h3><div class="widget"><ul id="recent-post" class="no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/fine_tune_part1/" class="title">总结Fine-Tune ChatGLM3的过程part-1</a></p><p class="item-date"><time datetime="2024-02-20T14:11:00.000Z" itemprop="datePublished">2024-02-20</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/fn_rag/" class="title">某销售场景下如何利用LLM</a></p><p class="item-date"><time datetime="2023-12-25T12:11:00.000Z" itemprop="datePublished">2023-12-25</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/Attention/" class="title">自注意力机制做了什么？</a></p><p class="item-date"><time datetime="2023-07-01T16:22:00.000Z" itemprop="datePublished">2023-07-01</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/AIGC_术语_A_Z/" class="title">AGI_Glossary术语</a></p><p class="item-date"><time datetime="2023-06-30T11:33:00.000Z" itemprop="datePublished">2023-06-30</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="article-category-link" href="/categories/Technical-Notes/">Technical Notes</a></p><p class="item-title"><a href="/LLM中的推理/" class="title">大模型中的推理</a></p><p class="item-date"><time datetime="2023-05-12T12:21:41.000Z" itemprop="datePublished">2023-05-12</time></p></div></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">categories</h3><div class="widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Technical-Notes/">Technical Notes</a><span class="category-list-count">22</span></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">archives</h3><div class="widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget-wrap widget-list"><h3 class="widget-title">tags</h3><div class="widget"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/">LLM</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance/">Performance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RAG/">RAG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/aliyun/">aliyun</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azure/">azure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ceph/">ceph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/">k8s</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/network/">network</a><span class="tag-list-count">2</span></li></ul></div></div></div></aside></div></div></div><footer id="footer"><div class="container"><div class="container-inner"><a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a><div class="credit"><h1 class="logo-wrap"><a href="/" class="logo"></a></h1><p>&copy; 2024 bjdzliu</p></div></div></div></footer><script>var disqus_shortname="hexo-theme-hueman",disqus_url="http://yoursite.com/fine_tune_part1/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script src="/js/main.js"></script></div></body>